{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from threading import Thread\n",
    "import multiprocessing as mp\n",
    "import queue as queue #import queue\n",
    "from networkx.algorithms import community\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "# my functions implemented in ./src/\n",
    "from vkprint import vkprint\n",
    "from crawling_algorythms import Crawler_RC,Crawler_RW,Crawler_DFS,Crawler_BFS,Crawler_MOD,Crawler_MED, METRICS_LIST\n",
    "from utils import import_graph,draw_graph,get_percentile,draw_percentile_heatmap\n",
    "from utils import draw_nodes_history,draw_scores_history,dump_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treading_crawler(seed_num, big_graph, crawling_method,node_seed, b, percentile_set):\n",
    "    \"\"\"\n",
    "    Paralleling algorythm. 1 process for 1 seed. Using global multiprocessing.Queue for export results\n",
    "    :param seed_num:\n",
    "    :param big_graph:\n",
    "    :param crawling_method:\n",
    "    :param node_seed:\n",
    "    :param b:\n",
    "    :param percentile_set:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    #global q\n",
    "    #print('thread alive', seed_num, q.qsize())\n",
    "    thread_crawler = crawling_method(big_graph, node_seed=node_seed, budget=b, percentile_set=percentile_set)\n",
    "\n",
    "    counter = 0\n",
    "    #thread_dump_file = open('../results/dumps/' + graph_name + '/method'+str(methods['RW'])[-5:-2]+seed'+str(node_seed)+str(b)+'.json', 'a+')\n",
    "    #thread_dump_file.write('[')\n",
    "    while counter < b:\n",
    "        if counter % 10000 == 1:\n",
    "            print(crawling_method,  counter, seed_num)  \n",
    "            thread_dump_file = open('../results/dumps/'+ graph_name + '/method'+str(crawling_method)[-5:-2]+'seed'+str(node_seed)+'iter_'+str(counter)+'|'+str(b)+'.json', 'w')\n",
    "            json.dump(thread_crawler.observed_history,thread_dump_file)\n",
    "            thread_dump_file.close()                   \n",
    "        counter += 1\n",
    "        thread_crawler.sampling_process()\n",
    "                   \n",
    "        # if counter %100:\n",
    "        # print(counter)\n",
    "    # print(seed_num,thread_crawler.observed_history)\n",
    "    # return_dict[seed_num] = thread_crawler.observed_history\n",
    "    vkprint('thread ready', seed_num, q.qsize())\n",
    "    q.put(thread_crawler.observed_history)\n",
    "    #print(seed_num, 'putted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m</home/denisaivazov/anaconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-745>\u001b[0m in \u001b[0;36mwrite_gml\u001b[0;34m(G, path, stringizer)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/networkx/utils/decorators.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(func_to_be_decorated, *args, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mclose_fobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m                 \u001b[0mfobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## макропараметры всей задачи\n",
    "graph_name = 'github'\n",
    "methods = {'RW':Crawler_RW,'RC':Crawler_RC,'DFS':Crawler_DFS, 'BFS':Crawler_BFS, \n",
    "              'MOD':Crawler_MOD,'MED':Crawler_MED} #'AFD',\n",
    "b = 300000\n",
    "seed_count =8\n",
    "top_percetile = 10\n",
    "\n",
    "# общие множества и вспомогательное, лучше не менять\n",
    "\n",
    "graph_name_list = ['importing','wikivote','hamsterster','DCAM','gnutella',  'dblp2010',  'github']  #'slashdot',\n",
    "linestyles = [':', '--', '-.','--']*seed_count\n",
    "\n",
    "\n",
    "# Дальше лучше не трогать\n",
    "t00 = time.time()\n",
    "big_graph = import_graph(graph_name)\n",
    "b = min(big_graph.number_of_nodes(),b)\n",
    "percentile, percentile_set = get_percentile(big_graph,graph_name,top_percetile) # берём топ 10 процентов вершин\n",
    "if graph_name == 'gnutella': # большой костыль.Мы брали не тот эксцентриситет\n",
    "    percentile_set['eccentricity']=set(big_graph.nodes()).difference(percentile_set['eccentricity'])\n",
    "\n",
    "nx.write_gml(big_graph, \"../data/Graphs/\"+ graph_name +'_BIG.graph')\n",
    "vkprint(\"big_graph \"+graph_name+\"\\nb=\",b,\"seed count=\",seed_count, \"methods:\",methods, \"\\n percentile\",percentile)\n",
    "\n",
    "\n",
    "#draw_graph(big_graph,graph_name) # отрисовываем граф, если не очень большой\n",
    "seeds = random.sample(set(big_graph.nodes),seed_count) # список начальных вершин, по которым мы будем проходиться\n",
    "crawler = dict()   \n",
    "crawler_avg = dict()\n",
    "\n",
    "seeds = random.sample(set(big_graph.nodes),seed_count)\n",
    "history = dict((method, []*seed_count) for method in methods)\n",
    "crawler = dict((method, {'nodes':[],'degrees':[],'k_cores':[],'eccentricity':[],\n",
    "                                  'betweenness_centrality':[]}) for method in methods)\n",
    "crawler_avg = dict((method, {'nodes':[],#np.zeros([len(methods)*seed_count,b+1]),\n",
    "                                  'degrees':[],'k_cores':[],'eccentricity':[],'betweenness_centrality':[]}) for method in methods)\n",
    "\n",
    "\n",
    "\n",
    "# для каждого метода отдельный список тредов, и всё в виде словаря\n",
    "threads =dict()\n",
    "\n",
    "for method in methods:   \n",
    "    #process_queue = list(seeds)    \n",
    "    #while len(process_queue)>0:\n",
    "    crawler_avg[method] = dict({i: np.zeros(b) for i in METRICS_LIST}, **{'nodes': np.zeros(b)})\n",
    "    property_history_hist = {'nodes':[],'degrees':[],'k_cores':[],'eccentricity':[],'betweenness_centrality':[]}\n",
    "    \n",
    "    threads [method] = []\n",
    "    \n",
    "    for seed_num in range(seed_count):\n",
    "        #process_method = process_queue.pop()\n",
    "        t0 =time.time()\n",
    "        #algorythm = methods[method]\n",
    "        \n",
    "    \n",
    "    q = mp.Queue()\n",
    "    for seed_num in range(seed_count):\n",
    "        threads[method].append(mp.Process(daemon = True,target=treading_crawler, args=(seed_num,big_graph,methods[method],seeds[seed_num],b,percentile_set)))\n",
    "        #threads[method][seed_num].setDaemon = True\n",
    "        threads[method][seed_num].start()\n",
    "        \n",
    "    for seed_num in range(seed_count):\n",
    "        history[method].append(q.get())\n",
    "        \n",
    "    q.close()\n",
    "    q.join_thread()    \n",
    "    \n",
    "    \n",
    "    for seed_num in range(seed_count):\n",
    "        threads[method][seed_num].join()\n",
    "        for prop in METRICS_LIST:\n",
    "            property_history_hist[prop] = np.array([i for i in history[method][seed_num][prop]])\n",
    "            crawler_avg[method][prop] += property_history_hist[prop]\n",
    "        \n",
    "        crawler_avg[method]['nodes'] += np.array(history[method][seed_num]['nodes'])\n",
    "        \n",
    "    #dump_results(graph_name,big_graph,crawler_avg,history)\n",
    "    vkprint(graph_name, seed_count, method, seed_num,' it took ', round(((time.time() -t0)/60),3),'minutes'  )\n",
    "dump_results(graph_name,crawler_avg,history,b)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8fa295450363>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mpercentile_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eccentricity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbig_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpercentile_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eccentricity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdraw_nodes_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcrawler_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethods\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgraph_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpercentile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercentile_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_percentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbig_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgraph_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtop_percetile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# берём топ 10 процентов вершин\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdraw_scores_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpercentile_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcrawler_avg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmethods\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgraph_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "if graph_name == 'gnutella': # большой костыль.Мы брали не тот эксцентриситет\n",
    "    percentile_set['eccentricity']=set(big_graph.nodes()).difference(percentile_set['eccentricity'])\n",
    "\n",
    "draw_nodes_history(history,crawler_avg, methods,graph_name,seed_count,b)\n",
    "percentile, percentile_set = get_percentile(big_graph,graph_name,top_percetile) # берём топ 10 процентов вершин\n",
    "draw_scores_history(percentile_set,crawler_avg,methods,graph_name,seed_count,b)\n",
    "draw_percentile_heatmap(percentile_set,graph_name,seed_count,b, normalized=True, venn_on=True)\n",
    "\n",
    "\n",
    "vkprint('i have done everything!!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
